{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ecc5a3-09ac-42e5-a337-6e0a2caf935c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb14aabb-908b-46b1-ac70-5888a765e5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np #for scientific computation\n",
    "import pandas as pd #for data analysis and data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd162d6-128d-4f0f-be64-569d0630f49b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define the schema for dataset\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType, StructType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"longitude\",DoubleType(),True),\n",
    "            StructField(\"latitude\",DoubleType(),True),\n",
    "            StructField(\"housing_median_age\",DoubleType(),True),\n",
    "            StructField(\"total_rooms\",DoubleType(),True),\n",
    "            StructField(\"total_bedrooms\",DoubleType(),True),\n",
    "            StructField(\"population\",DoubleType(),True),\n",
    "            StructField(\"households\",DoubleType(),True),\n",
    "            StructField(\"median_income\",DoubleType(),True),\n",
    "            StructField(\"median_house_value\",DoubleType(),True),\n",
    "            StructField(\"ocean_proximity\",StringType(),True)\n",
    "            ])\n",
    "\n",
    "#Read the data\n",
    "housing_df = spark.read.csv(\"dbfs:/FileStore/housing_data_tweaked.csv\",schema=schema)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505a5a86-82c8-4e0b-9ae1-fef479121e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(housing_df)\n",
    "#Output shows it is a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35857c55-e6f1-4f49-9cb9-136bebb127d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df = housing_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7677bffe-8fef-4c79-8039-76595d3e0506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(housing_df)\n",
    "\n",
    "#Now the dataframe type changed to pandas type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639362f9-553d-4d7b-95e8-da49c68d73ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3595f8c0-c1c6-4416-9f2c-d2a777e8048b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Missing value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bea3ebd-cbfc-4080-b4c3-5f19b75ba469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Missing value imputation is the process of replacing missing data in a dataset with estimated values. It's a key step in data preprocessing. \n",
    "\n",
    "**Why is imputation important?**\n",
    "\n",
    "Missing values can affect the accuracy of data analysis and machine learning models. \n",
    "\n",
    "If incomplete datasets are not imputed well, the results of data mining and analysis could be affected. \n",
    "\n",
    "**How is imputation done?**\n",
    "\n",
    "**_Mean imputation_**\n",
    "Replaces missing values with the average of the observed values for that variable. \n",
    "\n",
    "**_Median imputation_**\n",
    "Replaces missing values with the median of the observed values for that variable. \n",
    "\n",
    "**_Mode imputation_**\n",
    "Replaces missing values with the most frequent value (mode) of the variable. \n",
    "\n",
    "**_K-Nearest Neighbors (KNN) imputation_**\n",
    "Estimates missing values by finding the K most similar samples in the dataset and using their values to impute the missing data. \n",
    "\n",
    "**When should imputation be used? **\n",
    "\n",
    "Imputation should not be used if more than 50% of data are missing.\n",
    "\n",
    "Imputation is not appropriate for imputing future data in a time series.\n",
    "\n",
    "Use of imputation is suspect if it generates values outside valid ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "882bd7ed-d19a-42d0-8349-6b7d18529ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d5f55f-8486-4e61-8a39-c056168a2439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fill the missing values for numerical columns\n",
    "housing_df = housing_df.fillna(housing_df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc948bd9-9f52-4ed0-ae79-3e63f84580b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f630a28e-a51e-4c61-a4b0-eff4b660b7e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fill the missing values for Categorical columns\n",
    "housing_df.select_dtypes('object').columns\n",
    "\n",
    "#Replace the missing values with most frequent value; mode()[0] means we are accessing the first value\n",
    "housing_df['ocean_proximity'] = housing_df['ocean_proximity'].fillna(housing_df['ocean_proximity'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466a45f7-a4d1-4630-9715-430728be711b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0fab5c-8d82-4853-81cc-f37df48c6eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**What is Outlier?**\n",
    "\n",
    "An Outlier is a data item/object that deviates significantly from the rest of the (so-called normal) objects. Identifying outliers is important in statistics and data analysis because they can have a significant impact on the results of statistical analyses. The analysis for outlier detection is referred to as outlier mining.\n",
    "\n",
    "**Methods to remove outliers:**\n",
    "\n",
    "1. **Z score or standard score** \n",
    "\n",
    "Z score = (x -mean) / std. deviation\n",
    "\n",
    "A normal distribution is shown below and it is estimated that 68% of the data points lie between +/- 1 standard deviation. 95% of the data points lie between +/- 2 standard deviation 99.7% of the data points lie between +/- 3 standard deviation.If the **_z score of a data point is more than 3, it indicates that the data point is quite different from the other data points._** Such a data point can be an outlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4c5776-f536-48e9-9ab9-b68c5f3e988b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef38ba8c-c7ef-4d52-aec6-c444fbf58b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Define numerical columns\n",
    "\n",
    "#display(housing_df.select_dtypes(include=['float64','int64']).columns)\n",
    "#specify the columns explicitly in an array\n",
    "numerical_cols =['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income',\n",
    "       'median_house_value']\n",
    "#print(numerical_cols)\n",
    "\n",
    "#Step 2: Calculate Q1 (first quartile) and Q3 (third quartile) for each numerical column\n",
    "Q1 = housing_df[numerical_cols].quantile(0.25)\n",
    "Q3 = housing_df[numerical_cols].quantile(0.75)\n",
    "\n",
    "#Step 3: Calculate IQR for each numerical column\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#Step 4: Define lower and upper bounds for outlier detection\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "#Step 5: Filter out outliers from Dataframe\n",
    "housing_df_no_outliers = housing_df[~((housing_df[numerical_cols] < lower_bound) | (housing_df[numerical_cols] > upper_bound)).any(axis=1)] \n",
    "\n",
    "print(\"Before outlier removal\")\n",
    "display(housing_df.shape)\n",
    "initial_row_count = len(housing_df)\n",
    "print(\"Initial row count:\",initial_row_count)\n",
    "\n",
    "print(\"\\nAfter outlier removal\")\n",
    "display(housing_df_no_outliers.shape)\n",
    "final_row_count = len(housing_df_no_outliers)\n",
    "print(\"Final row count:\",final_row_count)\n",
    "\n",
    "rows_removed = initial_row_count - final_row_count\n",
    "print(\"Rows removed:\",rows_removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2923771b-d2d7-43ed-86c1-ae85dd917eb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a42e3a4-1b9c-4041-93b0-7ffceeafef8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Import package\n",
    "from scipy import stats\n",
    "\n",
    "#Step 2: Define numerical columns\n",
    "numerical_cols =['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income',\n",
    "       'median_house_value']\n",
    "\n",
    "#Step 3: Calculate z-scores for each numerical column\n",
    "z_scores = stats.zscore(housing_df[numerical_cols])\n",
    "\n",
    "#Step 4: Define threshold for outlier detection\n",
    "threshold = 3\n",
    "\n",
    "#Step 5: Filter out outliers from Dataframe\n",
    "housing_df_no_outliers = housing_df[(z_scores < threshold).all(axis=1)]\n",
    "\n",
    "print(\"Before outlier removal\")\n",
    "display(housing_df.shape)\n",
    "initial_row_count = len(housing_df)\n",
    "print(\"Initial row count:\",initial_row_count)\n",
    "\n",
    "print(\"\\nAfter outlier removal\")\n",
    "display(housing_df_no_outliers.shape)\n",
    "final_row_count = len(housing_df_no_outliers)\n",
    "print(\"Final row count:\",final_row_count)\n",
    "\n",
    "rows_removed = initial_row_count - final_row_count\n",
    "print(\"Rows removed:\",rows_removed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81a1c47b-d87b-4e90-9d56-c88f67982549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ae4e86-cd02-4d2d-88b9-1e72f4e71ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Feature creation is the process of creating new variables from existing data to help improve machine learning models. It's a common step in data preprocessing. \n",
    "\n",
    "Examples binning, splitting, calculated features, and creating new features from past values of time series data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b73ae38-10fb-44a1-9239-23410164a426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4604681c-4958-48cf-971f-35cdacb425c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New feature is created - housing_median_age_in_days with existing column\n",
    "housing_df[\"housing_median_age_in_days\"] = housing_df[\"housing_median_age\"] * 365\n",
    "\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b99d4ae-654f-4ea4-b1d0-de831bf4b80a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df.drop(columns=\"housing_median_age_in_days\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e30ea7-0625-4003-a871-9ae588f7294d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Do not run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7bb476c6-7042-4fda-854d-0df582400eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Polynomial feature\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "polynomial_features = poly.fit_transform(housing_df['feature_1', 'feature_2'])\n",
    "\n",
    "#Interaction features\n",
    "housing_df['interaction'] = housing_df['feature_1'] * housing_df['feature_2']\n",
    "\n",
    "#Binning/ discretization - To convert categorical data to numerical\n",
    "housing_df['binned_feature'] = pd.cut(housing_df['feature_1'], bins=3, labels=False)\n",
    "housing_df['age_group'] = pd.cut(housing_df['feature_1'], bins=[0, 20, 40, 60, 80, 100], labels=['Child','Young', 'Adult', 'Senior', 'Elderly'])\n",
    "\n",
    "#Encoding categorical features\n",
    "encode_df = pd.get_dummies(housing_df, columns= ['ocean_proximity'], prefix='ocean_proximity')\n",
    "\n",
    "#Textual features extraction using CountVectorizer for bag of words - For Natural Language Processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_data = ['text_1', 'text_2', 'text_3']\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(text_data)\n",
    "\n",
    "#time based feature\n",
    "housing_df['day_of_week'] = housing_df['date'].dt.dayofweek\n",
    "housing_df['month'] = housing_df['date'].dt.month\n",
    "\n",
    "#Domain specific features\n",
    "housing_df['Profit'] = housing_df['Revenue'] - housing_df['Cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d071f6f9-7b6a-472d-bbcf-23308bfddee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f27bd5ac-4659-4b2b-857c-ee9d3bd2d320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Feature Scaling:** Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data pre-processing step.\n",
    "\n",
    "2 common methods:\n",
    "\n",
    "**Z-Score Normalization :** Z-score normalization, also known as standardization, transforms data into a standard normal distribution with a mean of 0 and a standard deviation of 1. This technique is useful when the data follows a normal distribution.\n",
    "\n",
    "**Example:** Suppose we have a dataset with a feature \"Height\" with a mean of 175 cm and a standard deviation of 10 cm. To normalize this feature using z-score normalization, we would subtract the mean from each height and then divide by the standard deviation. This would result in a normalized feature with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**Min-Max Normalization** Min-max scaling, also known as rescaling, is a popular normalization technique that rescales the data to a common range, usually between 0 and 1. This is achieved by subtracting the minimum value and then dividing by the range of the data.\n",
    "\n",
    "**Example:** Suppose we have a dataset with a feature \"Age\" ranging from 18 to 80. To normalize this feature using min-max scaling, we would subtract 18 (the minimum value) from each age and then divide by 62 (the range of the data). This would result in a normalized feature with values between 0 and 1.\n",
    "\n",
    "**Why Scale?**\n",
    "\n",
    "Robustness to Outliers: Scaling can make your models less sensitive to extreme values.\n",
    "\n",
    "Algorithm Compatibility: Some algorithms, like Support Vector Machines and Principal Component Analysis, work best with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b9286c-091c-4f9c-bd0e-bbef2fa69b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Before applying feature scaling\")\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3696f9-a82a-44b2-8832-2550a10131b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Import packages\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Step 2: Define numerical columns\n",
    "numerical_features =['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income',\n",
    "       'median_house_value']\n",
    "\n",
    "#Step 3: Define function to scale numerical columns\n",
    "stdard_scaler = StandardScaler()\n",
    "housing_df[numerical_features] = stdard_scaler.fit_transform(housing_df[numerical_features])\n",
    "\n",
    "print(\"After applying feature scaling\")\n",
    "housing_df.head()\n",
    "#Numerical values are now scaled to common scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fa7671-af7e-41e2-8adc-cc8d415caa81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bd99a6-7ce3-41ee-bb37-a531d6630301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**One Hot Encoding** is a method for converting categorical variables into a binary format. It creates new columns for each category where 1 means the category is present and 0 means it is not. The primary purpose of One Hot Encoding is to ensure that categorical data can be effectively used in machine learning models.\n",
    "\n",
    "It is called one-hot encoding because only one column (or feature) corresponding to a particular category has the value 1, while all others are set to 0. For example, if the categories are Male and Female, the Male row will have [1, 0] and Female will have [0, 1]. This way, only one “hot” bit (1) is activated for each entry.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Wherever the fruit is “Apple,” the Apple column will have a value of 1 while the other fruit columns (like Mango or Orange) will contain 0.\n",
    "This pattern ensures that each categorical value gets its own column represented with binary values (1 or 0) making it usable for machine learning models.\n",
    "\n",
    "**Advantages of Using One Hot Encoding**\n",
    "\n",
    "It allows the use of categorical variables in models that require numerical input.\n",
    "\n",
    "It can improve model performance by providing more information to the model about the categorical variable.\n",
    "\n",
    "It can help to avoid the problem of ordinality which can occur when a categorical variable has a natural ordering (e.g. “small”, “medium”, “large”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08c1675-7e58-42fc-a5ce-868b8df34317",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e438ff51-9ba4-4e28-8820-c84d7d23a099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df['ocean_proximity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e55634-c8ae-4783-8dca-a980d699532e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df = pd.get_dummies(housing_df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbd47878-1738-4e28-9ad0-2422967bbbad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df.head() #new columns are added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a635bd8-92de-41a6-a530-5250d55c53f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "housing_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2597d0-8541-46b6-9405-c957fe5bcef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69162398-ef5e-4c12-aa84-ecbb3250c7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Feature Selection Foundation**\n",
    "\n",
    "Feature selection is a important step in machine learning which involves selecting a subset of relevant features from the original feature set to reduce the feature space while improving the model’s performance by reducing computational power. It’s a critical step in the machine learning especially when dealing with high-dimensional data.\n",
    "\n",
    "**There are various algorithms used for feature selection and are grouped into three main categories:**\n",
    "\n",
    "Filter Methods\n",
    "\n",
    "Wrapper Methods\n",
    "\n",
    "Embedded Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088d77f9-29d2-46df-9756-e1e16c7d8a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Store all independent variables in a dataframe X\n",
    "X = housing_df.drop('median_house_value', axis=1)\n",
    "\n",
    "#Store dependent variable in pandas series y\n",
    "y = housing_df['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c85b1429-e204-44b2-b64d-b4c093096ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9bc3bf4-8f3f-4b4f-a7db-cf2f36faffe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b79d09f-4015-4510-a76f-d098abdd0e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf803011-b59f-47ac-ad5d-203befeaab1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51cbd6f7-b770-4882-ba28-2b8672b908d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Create Feature selection model - Linear Regression estimator and 5 features to select\n",
    "estimator = LinearRegression()\n",
    "rfe = RFE(estimator, n_features_to_select=5)\n",
    "\n",
    "#Fit the feature selection model into data\n",
    "rfe.fit(X, y)\n",
    "\n",
    "#Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91e18e3-564f-4dd9-86b4-a490fe1e9b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7. Feature transformation (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1afc23-ab16-4b67-b887-2307b7c5991d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Feature transformation involves changing the original features in your dataset to new representations that may be more suitable for model building. This process can improve the performance of machine learning models by making the data more understandable and manageable for algorithms.\n",
    "\n",
    "The important transformation techniques we are going to read in this blog:\n",
    "\n",
    "Scaling- Normalization, Standardization\n",
    "\n",
    "Log Transformation\n",
    "\n",
    "Box Cox Transformation\n",
    "\n",
    "Encoding Categorical Variables\n",
    "\n",
    "Binning\n",
    "\n",
    "Reciprocal Transformation\n",
    "\n",
    "Polynomial Features\n",
    "\n",
    "Interaction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89afcc14-77be-44b0-95ae-e1a434c3ed80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "data = pd.DataFrame({\n",
    "        'feature1': [1, 2, 3, 4, 5],\n",
    "        'feature2': [10, 20, 30, 40, 50],\n",
    "        'feature3': [100, 200, 300, 400, 500]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d28216be-825e-44e9-a94b-448c667b293a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afde80bc-ec5f-4c7b-b84d-edf6cf8fa762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Normalization\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "#Standardization\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "#Logarthmic transformation\n",
    "log_data = np.log(data['feature1'])\n",
    "\n",
    "#Power Transformation\n",
    "power_data = np.sqrt(data['feature2'])\n",
    "\n",
    "#Boxcox Transformation\n",
    "boxcox_data = boxcox(data['feature3'])\n",
    "\n",
    "#Binning\n",
    "binned_data = pd.cut(data['feature3'], bins=3, labels=['low', 'medium', 'high'])\n",
    "\n",
    "#Polynomial Transformation\n",
    "poly_data = pd.DataFrame({\n",
    "    'feature_squared': data['feature1'] ** 2,\n",
    "    'feature_cubed': data['feature1'] ** 3\n",
    "})\n",
    "\n",
    "#Interaction terms\n",
    "interaction_data = data['feature1'] * data['feature2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6036f88a-1e9f-48ca-b9e9-00045d0ead6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Normalized data\", normalized_data)\n",
    "print(\"\\nStandardized data\", standardized_data)\n",
    "print(\"\\nLogarthmic data:\", log_data)\n",
    "print(\"\\nPower Transformation data:\", power_data)\n",
    "print(\"\\nBoxcox Transformation data:\", boxcox_data)\n",
    "print(\"\\nBinned data:\", binned_data)\n",
    "print(\"\\nPolynomial Transformation data:\", poly_data)\n",
    "print(\"interaction terms:\", interaction_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50061b63-ecc0-4db8-9a0c-f8e7d0a104b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 8. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9365787-f944-4c8c-bdb8-2bd165161925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dimensionality reduction is the process of reducing the number of features (or dimensions) in a dataset while retaining as much information as possible.\n",
    "\n",
    "two key approaches to reduce dimension:\n",
    "\n",
    "**1. Feature Selection:**  This method chooses the most relevant features from the dataset without altering them. It helps remove redundant or irrelevant features, improving model efficiency. There are several methods for feature selection including filter methods, wrapper methods, and embedded methods.\n",
    "\n",
    "**2. Feature Extraction** This method involves creating new features by combining or transforming the original features. There are several methods for feature extraction stated above in the introductory part which is responsible for creating and transforming the features. PCA is a popular technique that projects the original features onto a lower-dimensional space while preserving as much of the variance as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a007b1f-46ca-4cb5-aed3-c389dbf70e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Create PCA model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#Fit the PCA model into data\n",
    "pca.fit(X)\n",
    "\n",
    "#Transform X to new feature space\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "#Print shape of reduced feature space\n",
    "print(X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62c9faa-1736-4b23-9256-1aebea7943e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Print number of principle components\n",
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af06a53e-9b72-40de-8811-15130f61c269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(X_reduced)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Feature Engineering 2025-03-02 19:09:54",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
